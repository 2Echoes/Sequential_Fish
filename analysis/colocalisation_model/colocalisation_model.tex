\documentclass{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}

\title{Probabilistic model of random co-colocalization in a cell}
\author{}
\date{\today}

\begin{document}

\maketitle






\section{Aim}
For biological studies purpose we image a cell in 3D using fluorescent microscopy and we are interested in quantifying the 
interactions between different types of single molecules. From those images we have no means of quantifying directly the interaction
between single molecules, instead we want to see how frequently they localize together in space. Our goal is to build a general model
that will modelise the likelyness that a single molecule from distribution \textit{i} localize together or \textbf{co-localize} with
a single molecule from distribution \textit{j} in a cell of volume \textit{V} \textbf{assuming single molecules} 
\textbf{take random postions within the cells independently of one another}.

Our images are produced through a sequential fish microscope in fixed cells that divides, according to its resolution cells in a set of
\textit{v} voxels localized with their set of coordinates \textit{(z,y,x)} in the volume \textit{V}. 












\section{Co-localization events}
\subsection{Self co-localization event}
We note \textit{C(i,i)} a \textbf{self co-localization} event between two single molecules of distribution \textit{i} occupy the same
voxel.

\subsection{Co-localization event}
We note \textit{C(i,j)} a \textbf{co-localization} event between two single molecules of distribution \textit{i} and \textit(j)
when a single molecule from distribution \textit{i} occupy the same voxel as a single molecule from distribution \textit{j}.
















\newpage
\section{Probabilistic model}

\subsection{Presentation}
The process of assigning a position to single molecules amongst the $v$ possible positions can be modeled as a probabilistic game where
positions are uniquely numbered balls placed in an urn. Assigning coordinates to a single molecule is drawing a ball in the urn,
before each draw balls are replaced in the urn.
\subsection{Unique distribution}
To begin, let us consider a system containing a unique distribution $I$ of single molecules randomly placed amongst the $v$ positions of the cell.

\subsubsection{Self co-localization probability}
The probability that a specific single molecule $i$ is found at position $m$ is the probability to draw the ball numbered
$m$:

\[
    \hspace{3.5cm} p_i(X=m) = \frac{1}{v} \hspace{2cm} \forall  i \in I; \forall m \in V
\]
Then the probability to not draw the specific location \textit{m} is:
\[
    \hspace{3.5cm} p_i(\overline{m}) = 1-\frac{1}{v} \hspace{2cm} \forall  i \in I; \forall m \in V
\]
After \textit{k} draws, the probability to never draw a specific location \textit{m} follows a bionomial law and is :
\[
p_k(X_m = 0) = (1 - \frac{1}{v})^k \hspace{2cm} k \in \mathbb{N^*}
\]
On the contrary the probability that the location \textit{m} was drawn at least once is :
\begin{equation}
p_k(X_m \geq 1) = 1-(1 - \frac{1}{v})^k
\end{equation}
To study self co-localization we are interested to know how many different positions have been drawn in k-trials. To do so let us 
define the observation variable $\epsilon$.
\[
\forall m \in V, \epsilon (m) = \begin{cases}
    0 \text{ if } X_m = 0 \\
    1 \text{ if } X_m \geq 1
\end{cases}
\]
The expected number of \textbf{different} positions $ N^{unique}_{pos} $ drawn is then the expectancy of $ \epsilon $ in the volume after k draws.
\[
    \mu_{uniquepos} = \sum_{m \in V}p_k(X_m = 0).0 + p_k(X_m \geq 1).1
\]

\begin{equation}
    \begin{split}
    \mu_{up} = v(1-{(1-\frac{1}{v})}^k) \\
    \end{split}
\end{equation}
Where $\mu_{up}$ is the mean number of occupied position.
The expected number of different positions drawn is, in other words, the expected number of draws \textbf{that discovered a new position}.
To adress self co-localization probability we are interested to know the number of draws that \textbf{didn't} discover a new position,
in other words, duplicates draws or \textbf{self co-localization events}. To do so, we remove from the total number of picks (k) the
number of picks that discover a new position.
\[
\mu_{self colocalization} = k - v(1-(1-\frac{1}{v})^k)
\]
\begin{equation}
p_{self colocalization}  = \frac{\mu_{sc}}{k} = 1 - \frac{v}{k}(1-{(1-\frac{1}{v})}^k)
\end{equation}
\textit{\textbf{Notes:}}
\begin{itemize}
    \item This is true for any distribution \textit{I} of single molecule \textit{i} of abundancy $ k_i $ (i.e. number of single molecule).
    \item If $k = 1$, $p_{self colocalization} = 0$
\end{itemize}

\subsubsection{Self dependency of any distribution within the volume}

Although all distributions $i_i$ are independent from each other, $X_m$ and $X_l$ are \textbf{not independent} for
any pair of voxels $(m,l)$.$\epsilon$ follows a Bernoulli law, usually, summing this variable in the volume should
lead to a variable following a bionomial law. However we argue in this section that there is a co-dependency between
the voxels, meaning we can't consider the sum of $\epsilon (m)$ as abionomial distribution. 
To understand this let us consider the probability that a single molecule $i$ is found at voxel $m$ and at voxel $l \neq m$.
Starting with the inclusion-exclusion principle:
\[
P(X_m \geq 1 \cap X_l \geq 1) = P(X_m \geq 1) + P(X_l \geq 1) - P(X_m\geq 1 \cup X_l\geq 1)
\]
\[
P(X_m \geq 1 \cap X_l \geq 1) = 2p(X \geq 1) - P(X_m\geq 1 \cup X_l\geq 1)
\]
The probability that a single molecule localize at $l$ or $m$ is the chance to draw one of 2 voxels out of the volume $V$.
\[
P(m \cup l) = \frac{2}{v}
\]
Thus from:
\[
P({X_m\geq 1 \cup X_l\geq 1}) = 1 - (1-\frac{2}{v})^{k}
\]
We can already conclude $X_m$ and $X_l$ are not independent since their intersection is non null, it also hilights that the dependency is made through
the volume of voxels. To conclude calculus using above and \textbf{(1)} :
\begin{equation}
    P(X_m \geq 1 \cap X_l \geq 1) =  1 - 2(1-\frac{1}{v})^{k} + (1-\frac{2}{v})^{k} 
\end{equation}
\textbf{Conclusion : }All disitributions \textbf{are} independent from each other, but the occupancy of a voxel \textbf{is not} independent of the
occupancy of other voxels. This result is important for the computation of variance for the occupancy of voxel, since it means we must introduce a 
covariance term when summing up variables over voxels.

\subsubsection{Covariance between $I_m$ and $I_l$}

Covariance is defined as :
\[
    Cov(\epsilon(m);\epsilon(l)) = E(\epsilon(m) \cap \epsilon(l)) - E(\epsilon(m))E(\epsilon(l))
\]
where $E(\epsilon(m) \cap \epsilon(l))$ is the expectancy that positions $m$ \textbf{and} $l$ were drawn at least once in k draws. Let us start with left term  and 
using \textbf{(4)}:
\[
    E(\epsilon(m) \cap \epsilon(l)) = P(X_m \geq 1 \cap X_l \geq 1).1 = 1 - 2(1-\frac{1}{v})^{k} + (1-\frac{2}{v})^{k}
\]
For right term we have $E(\epsilon(m)) = E(\epsilon(l)) = E(\epsilon)$ :
\[
E(\epsilon) = P(X\geq1).1 = 1-(1 - \frac{1}{v})^k
\]
Finally :
\[
    Cov(\epsilon(m);\epsilon(l)) =  1 + (1-\frac{2}{v})^{k} - 2(1-\frac{1}{v})^{k} - [1-(1 - \frac{1}{v})^k]^2
\]
After simplification :
\begin{equation}
    Cov(\epsilon(m);\epsilon(l)) = (1-\frac{2}{v})^k - (1-\frac{1}{v})^{2k}
\end{equation}

\subsubsection{Standard deviation}
To fully determine the probability density of voxel occupancy, let us find its standard deviation. We are computing the variance from the sum of dependent variables
thus we have to use the law of total variance in its covariant form :
\[
Var(N_{up}) = \sum_{m \in \mathbb{J}} Var(\epsilon(m)) +  2\sum_{m < l }Cov(\epsilon(m);\epsilon(l))
\]
Where $\sum_{m<l}$ means we sum on all different \textbf{unordered} pairs for $m \neq l \in \mathbb{J}^2$.\newline
$I_m$ is a Bernoulli event of variance $p(1-p)$ :
\[
Var(\epsilon_m) = (1-(1-\frac{1}{v})^k)(1-\frac{1}{v})^k
\]
The number of unordered $(m,l)$ pairs for $l \neq m$ is the number of pairs of voxel we can fit in the $v$ voxels : 
\[
    2\sum_{m < l }Cov(I_m;I_l) = 2\binom{v}{2}Cov(I_m;I_l) = 2\frac{v(v-1)}{2}Cov 
\]

So total variance is :
\[
    \sigma^2_{up} = v[1-(1-\frac{1}{v})^k](1-\frac{1}{v})^k + v(v-1)[(1-\frac{2}{v})^k - (1-\frac{1}{v})^{2k}]
\]
And standard deviation :
\begin{equation}
    \sigma_{up} = \sqrt{v[1-(1-\frac{1}{v})^k](1-\frac{1}{v})^k + v(v-1)[(1-\frac{2}{v})^k - (1-\frac{1}{v})^{2k}]}
\end{equation}



\newpage
\subsection{Two distributions}
We now consider positions assignement of a second distribution \textit{I} of abundancy $ k_i $ after the assignement of a first 
distribution \textit{J} of abundancy $ k_j $ with $ I \neq J $. The number of unique positions \textit{J} occupy can be estimated with
\textbf{(2)}.
\[
N^j_{uniquepos} = v(1-(1-\frac{1}{v})^{k_j})
\]

A colocalisation event is drawing during assignement of $I$ a position that was already drawn when assigning positions to $J$. To
understand this in our probabilistic game, let us consider again a pool of $v$ uniquely numbered balls where all balls that were 
drawn while assigning postions to $J$ distribution have been colored in \textbf{red}. Again all balls are replaced in the pool after
each draw. Then the co-localization probability $p(C(i,j))$ is the
probability to draw a red ball.

\subsubsection{Probability of co-localization}
Drawing ball in the urn is a sequence of independent and uniformly random events. The probability of picking one of the $N^j_{uniquepos}$ red ball amongst the $v$
balls is :
\[
p(X\in \mathbb{J}) = \frac{N^j_{up}}{v}
\] 
where $\mathbb{J}$  is the set of positions drawn for the $J$ distribution.
If we consider distribution $J$ as fixed, co-localization events exactly follow a bionomial of parameters $(p(X\in\mathbb{J}),k_i)$. However, to consider a
more generalistic model, let us consider a distribution distribution $J$ of know abundancy $k_j$ with mean number of unique positions occupied following a
normal law $N(\mu_{sc}, \sigma_{sc})$. In such a case we use the law of total expectation :
\[
\mu_{Cij} = E(E(C(i,j) | J))
\]
$E(C(i,j) | J)$ is the expectancy from the bionomial law mentioned above.

\[
\rightarrow E(k_i\frac{N^j_{up}}{v}) = \frac{k_i}{v}E(N^j_{up})
\]

\begin{equation}
    \mu_{Cij} = \frac{k_i}(1-(1-\frac{1}{v})^{k_j})
\end{equation}

\textit{\textbf{Notes:}}
\begin{itemize}
    \item $\mu_{Cij} \neq \mu_{Cji}$
\end{itemize}

\subsubsection{Co-localization probability standard deviation}

Similarly to previous paragraph, let us find the standard deviation for number of co-localization events when considering $J$ not fixed. 
To do so the calculus of variance should take into consideration the variance of $N^j_{uniquepos}$ and use the law of total variance,
which states :
\[
Var(C(i,j)) = E[Var(C|J)] + Var(E[C|J])
\] 
$C|J$ is the case where distribution $J$ is fixed and co-localization events follow bionomial law (\S3.3.1) :

\[
    Var(C|J) = np(1-p) = k_i \frac{N^j_{uniquepos}}{v}(1-\frac{N^j_{uniquepos}}{v})
\]
\[
    E(C|J) = k_i \frac{N^j_{uniquepos}}{v}
\]
Now let us consider the first term with notation $N_j = N^j_{uniquepos}$
\[
    \begin{split}
    E[Var(C|J)] &= E(k_i \frac{N_j}{v}(1-\frac{N_j}{v})) \\
    &= \frac{k_i}{v}E(N_j - \frac{N^2_j}{v}) \\
    &= \frac{k_i}{v}[E(N_j) - \frac{E(N^2_j)}{v}] \\
    &= \frac{k_i}{v}(\mu^j_{up} - \frac{\sigma^2_{up} + \mu^2_{up}}{v})
\end{split}
\]
Using $Var(X) = E(X^2) - E^2(x)$.\newline
The other term is straightforward : 
\[
Var(E(C|J)) = k_i\frac{\sigma^{2_j}_{up}}{v}
\]
Total variance is :
\[
    \sigma^2_{Cij} = \frac{k_i}{v}(\mu^j_{up} - \frac{\sigma^2_{up} + \mu^2_{up}}{v}) + k_i\frac{\sigma^{2_j}_{up}}{v}
\]
With $\mu^j_{up}$ and $\sigma^j_{up}$ defined in equations \textbf{(2)} and \textbf{(6)}.\newline
In conclusion we can write the standard deviation of the number of co-localization events as :
\begin{equation}
    \sigma_{Cij} = \sqrt{\frac{k_i}{v}(\mu^j_{up} - \frac{\sigma^2_{up} + \mu^2_{up}}{v}) + k_i\frac{\sigma^{2_j}_{up}}{v}}
\end{equation}




\subsubsection{Voxel occupancy by a pair (i,j)}
In section 3.1 we deduced the probability that a specific location $m$ to be drawn at least once  in $k$ draws as :
\[
    \hspace{2cm} p_k(X_m \geq 1) = 1-(1 - \frac{1}{v})^k \hspace{2cm} \textbf{(1)}
\]
To know how many red balls have been drawn at least once (\textbf{i.e. number of unique pair (i,j)}) let us use again the observation
variable $\epsilon$.
\[
N^{(i,j)}_{up} = \sum_{m \in \mathbb{J}}p(C_m(i,j) \geq 1).1 + 0. \textbf{ ...}
\]
Which mean value can be found using the law of total expectation :
\begin{equation}
\mu^{(i,j)}_{up} = N^j_{up}(1-(1-\frac{1}{v})^{k_i})
\end{equation}
or for $J$ not fixed:
\[
\mu^{(i,j)}_{up} = \mu^j_{up}(1-(1-\frac{1}{v})^{k_i})
\]
\[
\mu^{(i,j)}_{up} = v(1-(1-\frac{1}{v})^{k_j})(1-(1-\frac{1}{v})^{k_i})
\]
\textit{\textbf{Notes:}}
\begin{itemize}
    \item Though $\mu_{Cij} \neq \mu_{Cji}$, explicit writting in last equation shows that \newline 
    $\mu^{(i,j)}_{up} = \mu^{(j,i)}_{up}$.
    \item Last equation also seems to show it will be easy to generalise the count of unique position occupied by any combination of molecules $c$ of dimension $n$
    as $N^c_{uniquepos} = v\prod_{i \in c}\mu_c$.
\end{itemize}




\subsubsection{Voxel occupancy by a pair (i,j)-Standard deviation}
For this calculus we have to consider again the variance of the voxel occupancy of the $J$ distribution in case it is not fixed. To do so we use again
the total variance formula in its generalistic form:
\[
    Var(N^{(i,j)}_{up}) = E[Var(N^{(i,j)}_{up}|J)] + Var(E[N^{(i,j)}_{up}|J])
\]
\textbf{Notes \& notations:}
\begin{itemize}
    \item Let $p_i = (1-{(1-\frac{1}{v})}^{k_i})$, the probability that voxel $m$ is occupied with at least one molecule $i$.
    \item The probability that $i$ and $j$ are in voxel $m$ can be written $P(X^i_m \cap X^j_m) = p_ip_j$ since $I$ and $J$ are independent.
    \item Let $q_i = (1-2{(1-\frac{1}{v})}^{k_i} + {(1-\frac{2}{v})}^{k_i})$, the probability that voxel $m$ and $l\neq m$ are both occupied with at least
    one molecule $i$.
    \item The probability that $i$ and $j$ are in voxel $m$ and $l$ can be written $P((X^i_m \cap X^j_m) \cap (X^i_l \cap X^j_l)) = q_iq_j$ since $I$ and $J$ are independent.
\end{itemize}

To make this less dense, let us break this in a few terms.\newline
The simplest term is obtained from \textbf{(9)}:
\[
E(N^{(i,j)}_{up}|J) = N^j_{up}p_i
\]
\[
    \Rightarrow Var(E(N^{(i,j)}_{up}|J)) = vp_iVar(N^j_{up})
\]
For the $Var(N^{(i,j)}_{up}|J)$ term we will have to take into consideration covariance term that arise from voxel-voxel dependancy (\S3.2.2). 
\[
    Var(N^{(i,j)}_{up}|J) = \sum_{m\in V}Var(\epsilon_{ij}(m)) + 2\sum_{m < l \in V}Cov(\epsilon_{ij}(m); \epsilon_{ij}(l))
\]

However we can take advantage that in this case the $J$ distribution is \textbf{fixed}, meaning that if we restrict
to the voxels having at least one $j$ particle (\textbf{i.e.} $\mathbb{J}$) the covariance arises only from the distribution $I$ and is exactly the same
that the one computed in \S3.2.3 when substuing $v$ with $N^j_{up}$.
\[\begin{split}
    2\sum_{m < l \in V}Cov(\epsilon_{ij}(m); \epsilon_{ij}(l)) &= 2\sum_{m < l \in \mathbb{J}}Cov(\epsilon_{ij}(m); \epsilon_{ij}(l)) \\
                & = N^j_{up}(N^j_{up}-1)[(1-\frac{2}{v})^k - (1-\frac{1}{v})^{2k}]
\end{split}
\]

The last term we should look at before summing-up is $\sum_{m\in V}Var(\epsilon_{ij}(m))$ using again $J$ fixed, we know $\epsilon_{i,j}(m) = 0 \hspace{0.20cm}\forall m \notin \mathbb{J}$:
\[\begin{split}
    \sum_{m\in V}Var(\epsilon_{ij}(m))&= \sum_{m\in \mathbb{J}}Var(\epsilon_{ij}(m))\\
                                      &=  \sum_{m\in \mathbb{J}}p_i(1-p_i) \\
                                      &=N^j_{up}p_i(1-p_i)
\end{split}
\]
Let us group everything together :
\[\begin{split}
    Var(N^{(i,j)}_{up}|J) &= \sum_{m\in V}Var(\epsilon_{ij}(m)) + 2\sum_{m < l \in V}Cov(\epsilon_{ij}(m); \epsilon_{ij}(l)) \\
                          &= N^j_{up}p_i(1-p_i)+ N^j_{up}(N^j_{up}-1)[(1-\frac{2}{v})^{k_i} - (1-\frac{1}{v})^{2k_i}]
\end{split}
\]
Applying expectancy to this term:
\[\begin{split}
E(Var(N^{(i,j)}_{up}|J)) &= vp_jp_i(1-p_i) + E((N^{2j}_{up} - N^j_{up})[(1-\frac{2}{v})^{k_i} - (1-\frac{1}{v})^{2k_i}]) \\
    &= vp_jp_i(1-p_i) + (vp_j(1-p_j) + v^2p^2_j - vp_j)[c_i] \\
    &= vp_jp_i(1-p_i) + (vp_j(1-p_j)c_i + v^2p^2_jc_i - vp_jc_i) \\
\end{split}
\]

\textbf{Notes \& notations}
\begin{itemize}
    \item Let $c_i = (1-\frac{2}{v})^{k_i} - (1-\frac{1}{v})^{2k_i}]$
    \item Note that $c_i + p^2_i = q_i$
    \item From \S3.2.4 we can write $Var(N^j_{up}) = vp_j(1-p_j) v(v-1)[q_j-p_j^2]$
\end{itemize}

Summing-up the total variance:
\[\begin{split}
    Var(N^{(i,j)}_{up}) &= vp_jp_i(1-p_i) + (vp_j(1-p_j)c_i + v^2p^2_jc_i - vp_jc_i) \\
        &+vp_iv(v-1)[q_j-p_j^2]) \\
\end{split}
\]
After simplification, we have the total variance:
\begin{equation}
    Var(N^{(i,j)}_{up}) = vp_ip_j(1-p_ip_j) + v(v-1)[q_iq_j-(p_ipj)^2]
\end{equation}

\newpage
\section*{Generalization to \(n\) Distributions}

Consider \(n\) independent molecule distributions labeled \(1,2,\dots,n\), with abundances \(k_i\) in a volume discretized into \(v\) voxels.

\subsection*{1. Single-Distribution Quantities}

For each distribution \(i\), define
\[
p_i \;=\;\Pr(X_{i,m}\ge1)
=1-\bigl(1-\tfrac1v\bigr)^{k_i},
\quad
q_i \;=\;\Pr(X_{i,m}\ge1,\;X_{i,\ell}\ge1)
=1 - 2\bigl(1-\tfrac1v\bigr)^{k_i} + \bigl(1-\tfrac2v\bigr)^{k_i},
\]
for any two distinct voxels \(m\neq\ell\).

\subsection*{2. Combination of Distributions}

Let \(c\subseteq\{1,\dots,n\}\) be any nonempty subset of distributions.  Define
\[
p_c \;=\;\Pr(\forall\,i\in c:\,X_{i,m}\ge1)
=\prod_{i\in c} p_i,
\quad
q_c \;=\;\Pr(\forall\,i\in c:\,X_{i,m}\ge1,\;X_{i,\ell}\ge1)
=\prod_{i\in c} q_i.
\]

\subsection*{3. Random Variable}

Define the count of voxels co-occupied by all distributions in \(c\):
\[
N_c \;=\;\sum_{m=1}^v I^{(c)}_m,
\qquad
I^{(c)}_m = \mathbf{1}\{\forall\,i\in c:\,X_{i,m}\ge1\}.
\]

\subsection*{4. Expectation}

Each \(I^{(c)}_m\) has marginal probability \(p_c\), so
\[
E\bigl[N_c\bigr]
= \sum_{m=1}^v E[I^{(c)}_m]
= v\,p_c.
\]

\subsection*{5. Variance and Standard Deviation}

Because the indicators \(I^{(c)}_m\) are dependent across voxels, use
\[
Var(N_c)
= \sum_{m=1}^v Var(I^{(c)}_m)
  + 2\sum_{1\le m<\ell\le v} Cov\bigl(I^{(c)}_m, I^{(c)}_\ell\bigr).
\]
Here,
\[
Var(I^{(c)}_m) = p_c(1-p_c),
\quad
Cov\bigl(I^{(c)}_m, I^{(c)}_\ell\bigr) = q_c - p_c^2.
\]
Thus
\[
Var(N_c)
= v\,p_c(1-p_c)
  + v(v-1)\bigl(q_c - p_c^2\bigr),
\]
and the standard deviation is
\[
\sigma(N_c)
= \sqrt{Var(N_c)} 
= \sqrt{\,v\,p_c(1-p_c) + v(v-1)\bigl(q_c - p_c^2\bigr)\,}.
\]


\newpage
\subsection{Generalisation to n distributions}

Let us consider n distributions: $i_1, i_2, \ldots, i_n$. What is the probality that any combination of single molecule $c$ to be found in an unique position $m$?
Under the assomption of randomess, any distribution $i$ is independent from other distributions, thus the probability to find $i_1$ and $i_2$ at any given
location is the product of the probabilities of distributions to localize at this location.

\[
p(c) = \prod_{i \in c}p(i)
\]
with 
\begin{equation}
\hspace{2cm} p(i) = 1-{(1-\frac{1}{v})}^{k_i} \hspace{2cm} \textbf{(i)}
\end{equation}

Again to know how many different positions contains a combination of single molecule $c$, we define the observation variable $I$.
\[
\forall m \in V, I_c(m) = \begin{cases}
    0 \text{ if } X^c_m = 0 \\
    1 \text{ if } X^c_m \geq 1
\end{cases}
\]



\begin{figure}[h]
\centering
\end{figure}

\end{document}  % End of the document
